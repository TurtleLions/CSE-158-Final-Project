{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import ast\n",
    "import random\n",
    "import gzip\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score, roc_curve\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4570395",
   "metadata": {},
   "source": [
    "# Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224053a3",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5d231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data\")\n",
    "data = []\n",
    "with gzip.open('australian_users_items.json.gz', 'rt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(ast.literal_eval(line))\n",
    "\n",
    "interactions = []\n",
    "for user in data:\n",
    "    uid = user['user_id']\n",
    "    u_items = user['items_count']\n",
    "    for item in user['items']:\n",
    "        interactions.append({\n",
    "            'user_id': uid,\n",
    "            'item_id': item['item_id'],\n",
    "            'playtime_forever': item['playtime_forever'],\n",
    "            'user_items_count': u_items\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(interactions)\n",
    "df_positive = df[df['playtime_forever'] > 0].copy()\n",
    "df_positive['target'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c788b85c",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neg sampling\n",
    "print(\"Generating neg samples\")\n",
    "\n",
    "top_games = df_positive['item_id'].value_counts().head(500).index.tolist()\n",
    "\n",
    "played_pairs = set(zip(df_positive['user_id'], df_positive['item_id']))\n",
    "users = df_positive['user_id'].values\n",
    "n_samples = len(users)\n",
    "\n",
    "negative_items = np.empty(n_samples, dtype=object)\n",
    "filled_mask = np.zeros(n_samples, dtype=bool)\n",
    "retry_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c8c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not filled_mask.all() and retry_count < 100:\n",
    "    missing = (~filled_mask).sum()\n",
    "    # Sample only from Top 500 games\n",
    "    candidates = np.random.choice(top_games, size=missing)\n",
    "    current_users = users[~filled_mask]\n",
    "    \n",
    "    valid_mask = np.array([(u, i) not in played_pairs for u, i in zip(current_users, candidates)])\n",
    "    \n",
    "    if valid_mask.any():\n",
    "        idx = np.where(~filled_mask)[0][valid_mask]\n",
    "        negative_items[idx] = candidates[valid_mask]\n",
    "        filled_mask[idx] = True\n",
    "    retry_count += 1\n",
    "    if retry_count % 10 == 0:\n",
    "        print(f\"Pass {retry_count}: {filled_mask.sum()}/{n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fffe788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative = pd.DataFrame({\n",
    "    'user_id': users,\n",
    "    'item_id': negative_items,\n",
    "    'user_items_count': df_positive['user_items_count'].values,\n",
    "    'target': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6926f84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "df_model = pd.concat([df_positive[['user_id', 'item_id', 'user_items_count', 'target']], \n",
    "                      df_negative[['user_id', 'item_id', 'user_items_count', 'target']]], ignore_index=True)\n",
    "df_model = df_model.dropna().sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d230f331",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c899750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "train_df, test_df = train_test_split(df_model, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Engineering\n",
    "print(\"Calculating features\")\n",
    "train_pos = train_df[train_df['target'] == 1]\n",
    "pop_map = train_pos.groupby('item_id')['user_id'].nunique().reset_index()\n",
    "pop_map.columns = ['item_id', 'item_popularity']\n",
    "\n",
    "train_df = train_df.merge(pop_map, on='item_id', how='left').fillna(0)\n",
    "test_df = test_df.merge(pop_map, on='item_id', how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9228a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['user_items_count', 'item_popularity']\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['target']\n",
    "X_test = test_df[features]\n",
    "y_test = test_df['target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccca80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Model\")\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b047f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = model.predict_proba(X_test_scaled)[:, 1]\n",
    "auc = roc_auc_score(y_test, y_probs)\n",
    "pr_auc = average_precision_score(y_test, y_probs)\n",
    "\n",
    "print(f\"ROC AUC: {auc:.4f}\")\n",
    "print(f\"PR AUC:  {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b5060",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "\n",
    "# plotting\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Game Prediction')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('roc_curve_base.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ae0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=features, y=np.abs(model.coef_[0]))\n",
    "plt.title('Feature Importance (Coefficient Magnitude)')\n",
    "plt.ylabel('Absolute Coefficient')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance_base.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2968c063",
   "metadata": {},
   "source": [
    "# Improved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff3623",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TEXT_FEATURES = 500\n",
    "MAX_TAG_FEATURES = 100\n",
    "EMBEDDING_DIM = 32\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "\n",
    "def smart_open(file_path, mode='rt', encoding='utf-8'):\n",
    "    if os.path.exists(file_path):\n",
    "        if file_path.endswith('.gz'):\n",
    "            return gzip.open(file_path, mode=mode, encoding=encoding)\n",
    "        return open(file_path, mode=mode, encoding=encoding)\n",
    "    \n",
    "    gz_path = file_path + '.gz'\n",
    "    if os.path.exists(gz_path):\n",
    "        return gzip.open(gz_path, mode=mode, encoding=encoding)\n",
    "    return open(file_path, mode=mode, encoding=encoding)\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text: return \"\"\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c80779",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aef6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load User/Item Interactions\n",
    "print(\"Loading interactions\")\n",
    "user_data = []\n",
    "with smart_open('australian_users_items.json.gz') as f:\n",
    "    for line in f:\n",
    "        user_data.append(ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Game Metadata\n",
    "print(\"Loading game metadata\")\n",
    "games_dict = {}\n",
    "with smart_open('steam_games.json.gz') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            game = ast.literal_eval(line)\n",
    "            gid = None\n",
    "            if 'id' in game: gid = str(game['id'])\n",
    "            elif 'app_id' in game: gid = str(game['app_id'])\n",
    "            if not gid: continue\n",
    "            \n",
    "            # Extract Price\n",
    "            price = game.get('price', 0)\n",
    "            if isinstance(price, str):\n",
    "                price = 0 if 'free' in price.lower() else float(price) if price.replace('.', '', 1).isdigit() else 0\n",
    "            \n",
    "            # Extract Tags\n",
    "            tags = game.get('tags', []) + game.get('genres', [])\n",
    "            tags_str = \" \".join([t.lower() for t in tags])\n",
    "            \n",
    "            games_dict[gid] = {'price': price, 'tags': tags_str}\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ead2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Reviews\n",
    "print(\"Loading reviews\")\n",
    "user_reviews_text = defaultdict(list)\n",
    "with smart_open('steam_reviews.json.gz') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            node = ast.literal_eval(line)\n",
    "            # Resolve ID\n",
    "            uid = str(node.get('user_id', node.get('username', '')))\n",
    "            if not uid: continue\n",
    "            \n",
    "            text = node.get('text', '')\n",
    "            if text:\n",
    "                user_reviews_text[uid].append(clean_text(text))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Collapse reviews into single string per user\n",
    "user_reviews_map = {uid: \" \".join(texts) for uid, texts in user_reviews_text.items()}\n",
    "del user_reviews_text\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0d822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df creation\")\n",
    "\n",
    "interactions = []\n",
    "for user in user_data:\n",
    "    uid = str(user['user_id'])\n",
    "    for item in user['items']:\n",
    "        interactions.append({\n",
    "            'user_id': uid,\n",
    "            'item_id': str(item['item_id']),\n",
    "            'playtime': item['playtime_forever']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(interactions)\n",
    "del user_data, interactions\n",
    "gc.collect()\n",
    "\n",
    "# Filter only played games and existing games\n",
    "df = df[df['playtime'] > 0]\n",
    "df = df[df['item_id'].isin(games_dict.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90913b8",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c2f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate popularity\n",
    "pop_counts = df['item_id'].value_counts().to_dict()\n",
    "df['popularity'] = df['item_id'].map(pop_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling\n",
    "print(\"Creating negative samples\")\n",
    "# Get list of all users and top 500 popular items\n",
    "all_users = df['user_id'].unique()\n",
    "top_items = list(df['item_id'].value_counts().head(500).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of existing pairs for fast lookup\n",
    "existing_pairs = set(zip(df['user_id'], df['item_id']))\n",
    "\n",
    "n_positives = len(df)\n",
    "target_negatives = n_positives\n",
    "\n",
    "unique_users = df['user_id'].unique()\n",
    "top_items_array = np.array(top_items)\n",
    "existing_pairs = set(zip(df['user_id'], df['item_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_rows = []\n",
    "\n",
    "# Generate batches until we hit the target\n",
    "while len(negative_rows) < target_negatives:\n",
    "    needed = target_negatives - len(negative_rows)\n",
    "    \n",
    "    batch_size = int(needed * 1.2)\n",
    "    \n",
    "    batch_u = np.random.choice(unique_users, size=batch_size)\n",
    "    batch_i = np.random.choice(top_items_array, size=batch_size)\n",
    "    \n",
    "    # Filter valid negatives\n",
    "    for u, i in zip(batch_u, batch_i):\n",
    "        if len(negative_rows) >= target_negatives:\n",
    "            break\n",
    "            \n",
    "        if (u, i) not in existing_pairs:\n",
    "            negative_rows.append({\n",
    "                'user_id': u,\n",
    "                'item_id': i,\n",
    "                'playtime': 0,\n",
    "                'popularity': pop_counts.get(i, 0)\n",
    "            })\n",
    "            # Add to existing_pairs to ensure we don't accidentally add the same negative twice\n",
    "            existing_pairs.add((u, i))\n",
    "\n",
    "df_neg = pd.DataFrame(negative_rows)\n",
    "df_neg['target'] = 0\n",
    "df['target'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3001e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Positive and Negative\n",
    "full_df = pd.concat([df[['user_id', 'item_id', 'popularity', 'target']], \n",
    "                     df_neg[['user_id', 'item_id', 'popularity', 'target']]], ignore_index=True)\n",
    "\n",
    "# Shuffle\n",
    "full_df = full_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset Size: {len(full_df)} interactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53992761",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features\")\n",
    "\n",
    "# ID Encoding (Label Encoding)\n",
    "print(\"Encoding IDs\")\n",
    "user_encoder = LabelEncoder()\n",
    "full_df['user_idx'] = user_encoder.fit_transform(full_df['user_id'])\n",
    "n_users = len(user_encoder.classes_)\n",
    "\n",
    "item_encoder = LabelEncoder()\n",
    "full_df['item_idx'] = item_encoder.fit_transform(full_df['item_id'])\n",
    "n_items = len(item_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7068f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF on User Reviews\n",
    "print(\"Vectorizing reviews\")\n",
    "reviews_series = full_df['user_id'].map(lambda u: user_reviews_map.get(u, \"\"))\n",
    "tfidf_rev = TfidfVectorizer(max_features=MAX_TEXT_FEATURES, stop_words='english', dtype=np.float32)\n",
    "X_reviews = tfidf_rev.fit_transform(reviews_series).toarray().astype(np.float32)\n",
    "\n",
    "# TF-IDF on Game Tags\n",
    "print(\"Vectorizing game tags\")\n",
    "tags_series = full_df['item_id'].map(lambda i: games_dict.get(i, {}).get('tags', \"\"))\n",
    "tfidf_tags = TfidfVectorizer(max_features=MAX_TAG_FEATURES, stop_words='english', dtype=np.float32)\n",
    "X_tags = tfidf_tags.fit_transform(tags_series).toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1eb41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Features (Price & Popularity)\n",
    "print(\"Normalizing Numerical Features\")\n",
    "price_series = full_df['item_id'].map(lambda i: games_dict.get(i, {}).get('price', 0)).values.reshape(-1, 1)\n",
    "pop_series = full_df['popularity'].values.reshape(-1, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_nums = scaler.fit_transform(np.hstack([price_series, pop_series])).astype(np.float32)\n",
    "\n",
    "X_side = np.hstack([X_reviews, X_tags, X_nums])\n",
    "\n",
    "print(f\"Final Side Feature Dimension: {X_side.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d2d799",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011bd057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "del reviews_series, tags_series, X_reviews, X_tags, X_nums\n",
    "gc.collect()\n",
    "\n",
    "# Prepare inputs\n",
    "X_u = full_df['user_idx'].values\n",
    "X_i = full_df['item_idx'].values\n",
    "y = full_df['target'].values\n",
    "\n",
    "# Train/Test split\n",
    "X_u_train, X_u_test, X_i_train, X_i_test, X_s_train, X_s_test, y_train, y_test = train_test_split(\n",
    "    X_u, X_i, X_side, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dea69c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating model\")\n",
    "\n",
    "# Define inputs\n",
    "user_input = layers.Input(shape=(1,), name='user_input')\n",
    "item_input = layers.Input(shape=(1,), name='item_input')\n",
    "side_input = layers.Input(shape=(X_side.shape[1],), name='side_input')\n",
    "\n",
    "del X_side, X_u, X_i, y\n",
    "del full_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aab067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latent factor embeddings\n",
    "u_emb = layers.Embedding(n_users, EMBEDDING_DIM, embeddings_regularizer=regularizers.l2(1e-5))(user_input)\n",
    "i_emb = layers.Embedding(n_items, EMBEDDING_DIM, embeddings_regularizer=regularizers.l2(1e-5))(item_input)\n",
    "\n",
    "u_vec = layers.Flatten()(u_emb)\n",
    "i_vec = layers.Flatten()(i_emb)\n",
    "\n",
    "mf_layer = layers.Dot(axes=1)([u_vec, i_vec])\n",
    "\n",
    "dense_1 = layers.Dense(256, activation='relu')(side_input)\n",
    "dense_1 = layers.Dropout(0.4)(dense_1)\n",
    "dense_2 = layers.Dense(128, activation='relu')(dense_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fcceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the raw user/item vectors in the deep path too, to learn non-linear interactions\n",
    "concat = layers.Concatenate()([mf_layer, u_vec, i_vec, dense_2])\n",
    "\n",
    "# Final prediction layer\n",
    "pred_layer = layers.Dense(64, activation='relu')(concat)\n",
    "output = layers.Dense(1, activation='sigmoid')(pred_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8923da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[user_input, item_input, side_input], outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(name='precision')]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a994930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training\")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_u_train, X_i_train, X_s_train],\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=([X_u_test, X_i_test, X_s_test], y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7981997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict([X_u_test, X_i_test, X_s_test], batch_size=BATCH_SIZE)\n",
    "roc = roc_auc_score(y_test, preds)\n",
    "pr = average_precision_score(y_test, preds)\n",
    "\n",
    "print(f\"ROC AUC: {roc:.4f}\")\n",
    "print(f\"PR AUC:  {pr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7458167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(5):\n",
    "    p = preds[i][0]\n",
    "    actual = y_test[i]\n",
    "    print(f\"Predicted: {p:.4f} | Actual: {actual}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
